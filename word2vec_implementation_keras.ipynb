{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5cA0QFzVgC1HDneYXx9pp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lijin-durairaj-code-mode/deep-learning/blob/main/word2vec_implementation_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8oTo1bmKywM",
        "outputId": "075afd76-c368-451b-cf31-2e7b281b7dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# data visualisation and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#configure\n",
        "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
        "% matplotlib inline\n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "\n",
        "#nltk\n",
        "import nltk\n",
        "\n",
        "#preprocessing\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
        "\n",
        "# for part-of-speech tagging\n",
        "from nltk import pos_tag\n",
        "\n",
        "# for named entity recognition (NER)\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# vectorizers for creating the document-term-matrix (DTM)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# BeautifulSoup libraray\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re # regex\n",
        "\n",
        "#model_selection\n",
        "from sklearn.model_selection import train_test_split,cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "#preprocessing scikit\n",
        "# from sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n",
        "\n",
        "#classifiaction.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
        "\n",
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "#keras\n",
        "import keras\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "#gensim w2v\n",
        "#word2vec\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "rkAwCxi3brDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_frame=pd.read_csv(r'./sample_data/Reviews.csv')\n",
        "df=rev_frame.copy()\n",
        "df=df[['Text','Score']]\n",
        "df['review']=df['Text']\n",
        "df['rating']=df['Score']\n",
        "df.drop(['Text','Score'],axis=1,inplace=True)\n",
        "df.drop_duplicates(subset=['rating','review'],keep='first',inplace=True)\n",
        "\n",
        "def mark_sentiment(rating):\n",
        "  if(rating<=3):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "df['sentiment']=df['rating'].apply(mark_sentiment)\n",
        "df.drop(['rating'],axis=1,inplace=True)\n",
        "\n",
        "def clean_reviews(review):\n",
        "\n",
        "    # 1. Removing html tags\n",
        "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
        "\n",
        "    # 2. Retaining only alphabets.\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
        "\n",
        "    # 3. Converting to lower case and splitting\n",
        "    word_tokens= review_text.lower().split()\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    le=WordNetLemmatizer()\n",
        "    stop_words= set(stopwords.words(\"english\"))\n",
        "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
        "\n",
        "    cleaned_review=\" \".join(word_tokens)\n",
        "    return cleaned_review\n",
        "\n",
        "pos_df=df.loc[df.sentiment==1,:][:500]\n",
        "neg_df=df.loc[df.sentiment==0,:][:500]\n",
        "df=pd.concat([pos_df,neg_df],ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "cvjznc6sbxCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences=[]\n",
        "sum=0\n",
        "for review in df['review']:\n",
        "  sents=tokenizer.tokenize(review.strip())\n",
        "  sum+=len(sents)\n",
        "  for sent in sents:\n",
        "    cleaned_sent=clean_reviews(sent)\n",
        "    # print(cleaned_sent)\n",
        "    sentences.append(cleaned_sent.split())"
      ],
      "metadata": {
        "id": "1an4DhFzcMDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "w2v_model=gensim.models.Word2Vec(sentences=sentences,size=300,window=10,min_count=1)\n",
        "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))\n",
        "vocab=w2v_model.wv.vocab\n",
        "\n",
        "word_vec_dict={}\n",
        "for word in vocab:\n",
        "  word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
        "\n",
        "\n",
        "\n",
        "df['clean_review']=df['review'].apply(clean_reviews)\n",
        "\n",
        "maxi=-1\n",
        "for i,rev in enumerate(df['clean_review']):\n",
        "  tokens=rev.split()\n",
        "  if(len(tokens)>maxi):\n",
        "    maxi=len(tokens)\n"
      ],
      "metadata": {
        "id": "LMsc3VqkcRn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(df['clean_review'])\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "encd_rev = tok.texts_to_sequences(df['clean_review'])\n",
        "\n",
        "max_rev_len=maxi  # max lenght of a review\n",
        "vocab_size = len(tok.word_index) + 1  # total no of words\n",
        "embed_dim=300 # embedding dimension as choosen in word2vec constructor\n",
        "\n",
        "pad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
        "\n",
        "\n",
        "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
        "for word,i in tok.word_index.items():\n",
        "  embed_vector=word_vec_dict.get(word)\n",
        "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
        "    embed_matrix[i]=embed_vector"
      ],
      "metadata": {
        "id": "DKgYrEaMcenU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('df shape {0}'.format(df.shape))\n",
        "print('embed matrix shape {0}'.format(embed_matrix.shape))\n",
        "# print('pad rev shape {0}'.format(pad_rev.shape))\n",
        "print('vocab size {0}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6djQoj8hP_0H",
        "outputId": "fdc7b04f-335a-4338-fb50-afb739b53b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df shape (100000, 3)\n",
            "embed matrix shape (56380, 300)\n",
            "pad rev shape (100000, 1565)\n",
            "vocab size 56380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "IaD7VXUnQvQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_rev_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pKjjJ_dRMgH",
        "outputId": "27225df6-3cd3-47a6-fca2-e9a164a1d36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1565"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7StrudcaVZXU",
        "outputId": "ecf1f452-d5b4-43b3-ecd0-ca43013fe033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y=keras.utils.np_utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\n",
        "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)"
      ],
      "metadata": {
        "id": "qygE0C3lclqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers"
      ],
      "metadata": {
        "id": "OijGwKDXRbTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import Constant\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Dropout\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_rev_len,embeddings_initializer=Constant(embed_matrix)))\n",
        "# model.add(CuDNNLSTM(64,return_sequences=False)) # loss stucks at about\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dropout(0.50))\n",
        "# model.add(Dense(16,activation='relu'))\n",
        "# model.add(Dropout(0.20))\n",
        "model.add(Dense(2,activation='sigmoid'))  # sigmod for bin. classification.\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "epochs=2\n",
        "batch_size=64\n",
        "\n",
        "model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcBiAQc4cnf2",
        "outputId": "45a258bc-224a-42ea-b5c8-97a49ca8b217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 0.7715 - accuracy: 0.5125 - val_loss: 0.6885 - val_accuracy: 0.4800\n",
            "Epoch 2/2\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 0.6947 - accuracy: 0.5100 - val_loss: 0.6940 - val_accuracy: 0.4450\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2347467990>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_d=np.array(3)\n",
        "_d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRUkVm87cSWM",
        "outputId": "d3940fb0-3a11-4661-d990-bc39331aad70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(3)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_d.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uumjv2RmfUfR",
        "outputId": "e7a74091-e691-449d-9846-254a2b9f0743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}